{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import sklearn.metrics as metrics\n",
    "import time\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from xgboost import XGBClassifier, plot_tree\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import tree,svm\n",
    "from sklearn.metrics import classification_report, precision_score, recall_score,accuracy_score,confusion_matrix,roc_curve,auc\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV,cross_val_score, KFold\n",
    "from sklearn.base import TransformerMixin,BaseEstimator, ClassifierMixin\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "from IPython.display import display, clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generational Data Subset\n",
    "__Overview:__ In an earlier statistical analysis, we discovered a normally distributed generational trend with the mode at the age of 23 and drifting to 43 over the 20 year interval. The purpose of the generation_data() function is to get a subset of the patient data defined by one of two criteria:\n",
    "1. Include patient data involving age within 2 standard deviations of the norm of the yearly opiate presciption distribution over the specified time interval\n",
    "3. Only include patient data with age equal to 'start'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generation_data(df,marching=True,start=1,end=33,plott=False):\n",
    "    \n",
    "    '''\n",
    "    Input:\n",
    "    -----------\n",
    "    df        :  Pandas Dataframe - The data to get a subset of\n",
    "    marching  :  Boolean          - If True will return patients between the ages 'start' and 'end'\n",
    "                                    where each year 'start' and 'end' are incremented by 1\n",
    "    start     :  Integer          - Youngest age to include, if 'indiv' is set to 'True' then the \n",
    "                                    data will only include patients with age equal to the value of 'start'\n",
    "    end       :  Integer          - Oldest age to include\n",
    "    plott     :  Boolean          - If True, it will plot histograms of the newly created datasets\n",
    "    \n",
    "    Output:\n",
    "    -----------\n",
    "    generational : Pandas Dataframe - The modified data \n",
    "    '''\n",
    "    \n",
    "    years = list(set(df.YEAR))    # Get every year included in the dataset\n",
    "    generational = pd.DataFrame() # Create an empty dataframe\n",
    "    \n",
    "    # If indicated, Perform the marching sampling\n",
    "    if marching:\n",
    "      \n",
    "        for year in years: # Iterate through each year\n",
    "            \n",
    "            current_year = df[df.YEAR==year] # Get all patients involved in the current year\n",
    "            \n",
    "            # Remove everyone younger than 'start' and older than 'end'\n",
    "            generation = current_year[current_year['AGE'] >= start] \n",
    "            generation = generation[generation['AGE'] <= end]\n",
    "            \n",
    "            # Make generation into a dataframe\n",
    "            YEAR = pd.DataFrame(generation)\n",
    "            \n",
    "            # Only get the instances where people received opiates\n",
    "            rec = YEAR[YEAR['received2']==1]\n",
    "            \n",
    "            if plott: # If 'plott' is set to 'True'\n",
    "\n",
    "                rec.hist(['AGE']) # Then plot the instances of people receiving opiates\n",
    "                plt.show()\n",
    "\n",
    "                \n",
    "                rec = YEAR[YEAR['received2']==0] # and plot the instances of people not receiveing opiates\n",
    "                rec.hist(['AGE'])\n",
    "            \n",
    "            # Append the resulting data to 'generational'\n",
    "            generational = generational.append(YEAR)\n",
    "            \n",
    "            # Increment 'start' and 'end' to show patients have grown one year\n",
    "            start+=1\n",
    "            end+=1\n",
    "        \n",
    "        return generational\n",
    "    \n",
    "    # If 'marching' is 'False', it is assumed individual sampling is desired\n",
    "    else: \n",
    "        \n",
    "        for year in years:  # Iterate through each year\n",
    "            \n",
    "            # Get all patients involved in the current year\n",
    "            current_year = df[df.YEAR==year]\n",
    "            \n",
    "            # Get all patients with the age equal to 'start'\n",
    "            generation = current_year[current_year['AGE'] == start]\n",
    "            \n",
    "            # Make generation into a dataframe\n",
    "            YEAR = pd.DataFrame(generation)\n",
    "            \n",
    "            if plott: # If 'plott' is set to 'True'\n",
    "                \n",
    "                rec = YEAR[YEAR['received2']==1]  # Only get the instances where people received opiates\n",
    "                rec.hist(['AGE']) # Plot the instances of people receiving opiates\n",
    "                plt.show()\n",
    "\n",
    "                \n",
    "                rec = YEAR[YEAR['received2']==0] # Only get instances where people did not receive opiates\n",
    "                rec.hist(['AGE']) # Plot the instances of people not receiveing opiates\n",
    "                plt.show()\n",
    "            \n",
    "            # Put generation into a dataframe\n",
    "            generational = generational.append(pd.DataFrame(generation))\n",
    "            \n",
    "        return generational "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further Data Cleaning and Feature Engineering\n",
    "__Overview:__ The data_prep() function formats the data for machine learning purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def data_prep(df,N=False,smote=False,pre=False,prepre=True):\n",
    "    \n",
    "    '''\n",
    "    PURPOSE: \n",
    "        \n",
    "        Engineer features of the given dataframe and separate it into it's features and labels in prep\n",
    "        for machine learning\n",
    "    \n",
    "    PARAMETERS:\n",
    "    \n",
    "        df : Pandas Dataframe\n",
    "        N  : Boolean indicating whether df is the non_deterministic dataset(See Line #54)\n",
    "        smote : Boolean indicating wheter to perform SMOTE sampling     \n",
    "        \n",
    "    RETURNS:\n",
    "        \n",
    "        X_train : Training features\n",
    "        X_test  : Testing features\n",
    "        y_train : Training labels\n",
    "        y_test  : Testing labels\n",
    "        X       : The original dataset\n",
    "    '''\n",
    "    \n",
    "    if prepre:\n",
    "        df = df.fillna(0).sort_values(by=['YEAR'])  # Replace all NaN's with 0 and sort the values by the 'YEAR' feature\n",
    "        years = df.YEAR.tolist()                    # Store 'YEAR' column as a list called years\n",
    "        ages = df.AGE.tolist()                      # Store 'AGE' column as a list called ages \n",
    "\n",
    "        diagnoses = df.groupby(['DIAG1'])['AGE'].count().reset_index('DIAG1')                   # Store 'DIAG1' column as a dataframe called Diagnoses\n",
    "        top_diagnoses = diagnoses.sort_values(by='AGE', ascending=False)['DIAG1'].values[:20]   # Sort diagnoses from most prevalent to least prevalent\n",
    "\n",
    "\n",
    "        # If we have the non_deterministic dataset, drop the 'AGE' column\n",
    "        if N: \n",
    "            df = df.drop('AGE',axis=1)\n",
    "\n",
    "        df.loc[~df.DIAG1.isin(top_diagnoses),'DIAG1'] = '0'   # Set any diagnosis not in the top 20 to 0\n",
    "        df2 = pd.get_dummies(df[\"DIAG1\"], drop_first=True)    # Make string-encoded, categorical data (diagnosis) into dummies dataframe\n",
    "        df = pd.concat((df,df2),axis=1)                       # Combine dataframes\n",
    "        df = df.drop('DIAG1',axis=1)                          # Drop the 'DIAG1' column from df since we are done with it\n",
    "        df = df.dropna(axis = 0, how='any')                   # Drop any remaining rows with Nan's (if there are any)\n",
    "\n",
    "    if pre:\n",
    "        return df\n",
    "    \n",
    "    y = np.array(df['received2'])                         # The data labels are the 'received2' column of df\n",
    "    \n",
    "    X = df.drop(['received2','YEAR','REGION'],axis=1)     # The data features are everything else minus 'received2','YEAR','REGION' columns\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Perform a train test split on X and y\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=.33,shuffle=True,random_state=42)\n",
    "    \n",
    "    # If SMOTE sampling is requested,\n",
    "    if smote:\n",
    "        \n",
    "        # Build a SMOTE object\n",
    "        sm = SMOTE(random_state = 42)\n",
    "        \n",
    "        # Perform SMOTE sampling on the training data\n",
    "        X_train, y_train = sm.fit_resample(X_train,y_train)\n",
    "        \n",
    "        X_train = pd.DataFrame(X_train)\n",
    "        X_train.columns = X_test.columns\n",
    "        \n",
    "#         y_train = pd.DataFrame(y_train)\n",
    "#         y_train.columns = y_test.columns\n",
    "    \n",
    "    # Return the dataframe, features, and labels\n",
    "    return X_train,X_test,y_train,y_test,X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Model\n",
    "__Overview:__ The purpose of the Model() function is to create a model trained on the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Model(X_train,X_test,y_train,y_test,depth=11,estims=10,clf='RF'):\n",
    "    \n",
    "    ''' \n",
    "    input: \n",
    "        X      : Dataframe of dataset features\n",
    "        y      : Dataframe of dataset labels corresponding to X\n",
    "        smote  : Boolean indicating whether or not to use SMOTE sampling\n",
    "        depth  : Integer , if the model type is a forest, indicates maximum tree depth in the forest\n",
    "        estims : Integer indicating the number of estimators to use\n",
    "        clf    : String, 'RF','XGB' referring to the model to use\n",
    "    \n",
    "    output:\n",
    "        Model   : The model trained on the data\n",
    "        predict : The predictive scores of the model    \n",
    "    '''\n",
    "    \n",
    "    # If the requested model is a Random Forest\n",
    "    if clf == 'RF':\n",
    "        \n",
    "        # Build a Random Forest Classifier Object with maximum depth of depth and estims number of n_estimators\n",
    "        Model = RandomForestClassifier(max_depth=depth, n_estimators = estims)\n",
    "        \n",
    "        # Fit the model to our training set\n",
    "        Model.fit(X_train,y_train)\n",
    "        \n",
    "        # Predict the model on our test features\n",
    "        Predict = Model.predict(X_test)\n",
    "        \n",
    "    # If the requested model is a Boosted Random Forest\n",
    "    elif clf =='XGB':\n",
    "        \n",
    "        # Build a Boosted Random Forest Classifier with maximum depth of depth\n",
    "        Model = XGBClassifier(max_depth=depth)\n",
    "        \n",
    "        # Fit the model to the training set\n",
    "        Model.fit(X_train,y_train)\n",
    "        \n",
    "        # Predict the model on our test features\n",
    "        Predict = Model.predict(X_test)\n",
    "        \n",
    "    return Model,Predict\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scoring the Model\n",
    "__Overview:__ The calc_metrics() function returns a number of scores and metrics for the given model on used data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_metrics(X_test,y_test,Model,Predict,clf,roc=False):\n",
    "\n",
    "    '''\n",
    "    input:\n",
    "        X_test  : A dataframe of the test data\n",
    "        y_test  : A dataframe of the corresponding labels to X_test\n",
    "        Model   : The trained model using X_train, y_train\n",
    "        Predict : The scores of the model on the test data\n",
    "        clf     : A string - 'RF','XGB' determining the type of model used\n",
    "        roc     : A boolean, if True will generate the roc curve of Model\n",
    "        \n",
    "    output:\n",
    "        M           : the confusion matrix for the model\n",
    "        importances : a list of the importance of each feature in classifying the data\n",
    "        indices     : A list of the indices of each important feature, ranked by importance\n",
    "        Score       : The score of the model\n",
    "        fpr         : false positive rate\n",
    "        tpr         : true positive rate\n",
    "        roc_auc     : area under the roc curve\n",
    "    '''\n",
    "    \n",
    "    # Calculate the confusion matrix using our test labels and test predictions\n",
    "    M = confusion_matrix(y_test, Predict)\n",
    "\n",
    "    # Retrieve the most important features for classification fromt he model\n",
    "    importances = Model.feature_importances_\n",
    "    \n",
    "    # Get the indices of the important features\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "    \n",
    "    if clf=='XGB':\n",
    "        std = None\n",
    "    \n",
    "    # If the model was Random Forest\n",
    "    if clf=='RF':\n",
    "        \n",
    "        # Get the standard deviation of each feature importance\n",
    "        std = np.std([tree.feature_importances_ for tree in Model.estimators_],\n",
    "                 axis=0)\n",
    "  \n",
    "    # Get the model's score from the test data\n",
    "    Score = Model.score(X_test,y_test)\n",
    "\n",
    "    # If ROC Curve was requested\n",
    "    if roc:\n",
    "        \n",
    "        # Get the predictive probabilities of the X_test set\n",
    "        probs = Model.predict_proba(X_test)\n",
    "        \n",
    "        # Get the 2nd column of the probabilities list, this is the predictive rate\n",
    "        preds = probs[:,1]\n",
    "        \n",
    "        # Get the False Positive Rate, True Positive Rate, and Threshholds \n",
    "        fpr, tpr, thresh = roc_curve(y_test,preds)\n",
    "        \n",
    "        # Get the area under the ROC Curve\n",
    "        roc_auc = metrics.auc(fpr,tpr)\n",
    "    \n",
    "        return M,importances,indices,Score,fpr,tpr,roc_auc\n",
    "    return M,importances,indices,Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convey the Results of the Data\n",
    "__Overview:__ The Model_Report() function condenses the results of running the model and saves the data to a txt file, communicating it to the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Model_Report(name,y_train,Score,M,y_test,Predict,importances,indices,fpr,tpr,X,smote=False):\n",
    "    \"\"\"\n",
    "    input:\n",
    "        name        : String - the user input name of the model\n",
    "        y_train     : Dataframe - the labels for the train dataset\n",
    "        Score       : The score of the model on the test data\n",
    "        M           : Array - The confusion matrix for the model\n",
    "        y_test      : Dataframe - the labels for the test dataset\n",
    "        Predict     : The prediction score of the model\n",
    "        importances : List - the features ranked according to importance\n",
    "        indices     : List - the indices of each feature\n",
    "        fpr         : float - false positive rate\n",
    "        tpr         : float - true positive rate\n",
    "        X           : Dataframe - the train and test data \n",
    "        smote       : Boolean - If True will include SMOTE information\n",
    "        \n",
    "    output:\n",
    "        dom1 : the domain for the roc Curve\n",
    "        ran1 : the range for the roc curve\n",
    "    \"\"\"\n",
    "    \n",
    "    # Open the file for writing\n",
    "    f = open('REPORT FOR '+name,'w')\n",
    "    print('\\n\\n-----------------------------------------------------------')\n",
    "    f.write('\\n\\n-----------------------------------------------------------')\n",
    "    f.write('\\nREPORT FOR '+name)\n",
    "    if smote:\n",
    "        print('\\n\\nStats With Smote\\n\\n')\n",
    "        print('\\nSmote dataset shape %s \\n' % Counter(y_train))\n",
    "        f.write('\\n\\nStats With Smote\\n\\nSmote dataset shape %s \\n' % Counter(y_train))\n",
    "    else:\n",
    "        print('\\nStats Without Smote\\n')\n",
    "        print('Original dataset shape %s \\n' % Counter(y_train))\n",
    "        f.write('\\nStats Without Smote\\nOriginal dataset shape %s \\n' % Counter(y_train))\n",
    " \n",
    "    print('Score:\\t\\t\\t', round(Score,4))\n",
    "    print('\\n\\nCorrectly Predicted Not Received :\\t',round(M[0][0],4))\n",
    "    print('Incorrectly Predicted Not Received:\\t',round(M[0][1],4))\n",
    "    print('Accuracy Predicting Not Received:\\t',round(M[0][0]/(M[0][0]+M[0][1]),4))\n",
    "    print('\\n\\nCorrectly Predicted Received:\\t\\t',round(M[1][1],4))\n",
    "    print('Incorrectly Predicted Received:\\t\\t',round(M[1][0],4))\n",
    "    print('Accuracy Predicting Received:\\t\\t',round(M[1][1]/(M[1][0]+M[1][1]),4))\n",
    "    \n",
    "    print('\\n',classification_report(y_test,Predict))\n",
    "    \n",
    "    # Print the feature ranking\n",
    "    f.write('\\n\\nScore:\\t\\t\\t'+str(round(Score,4))+\n",
    "            '\\n\\nCorrectly Predicted Not Received :\\t'+str(round(M[0][0],4))+\n",
    "            '\\nIncorrectly Predicted Not Received:\\t'+str(round(M[0][1],4))+\n",
    "            '\\nAccuracy Predicting Not Received:\\t'+str(round(M[0][0]/(M[0][0]+M[0][1]),4))+\n",
    "            '\\n\\nCorrectly Predicted Received:\\t\\t'+str(round(M[1][1],4))+\n",
    "            '\\nIncorrectly Predicted Received:\\t\\t'+str(round(M[1][0],4))+\n",
    "            '\\nAccuracy Predicting Received:\\t\\t'+str(round(M[1][1]/(M[1][0]+M[1][1]),4))+\n",
    "            '\\n\\n'+str(classification_report(y_test,Predict))+\"\\n\\nFeature ranking:\")\n",
    "    \n",
    "    for i in range(10):\n",
    "        \n",
    "        f.write(\"\\n%d. %s \"% (i + 1, X.columns[indices[i]]))\n",
    "        f.write(str(round(100*importances[indices[i]],3))+'%')\n",
    "        \n",
    "    # Plot the feature importances of the forest in a bar graph\n",
    "    plt.figure()\n",
    "    plt.title(\"Feature importances\")\n",
    "    plt.bar(range(X.shape[1]), importances[indices], color=\"r\", align=\"center\")\n",
    "    plt.xticks(range(10), X.columns[indices],rotation='vertical')\n",
    "    plt.xlim([-1,9.6])\n",
    "    plt.savefig(name+' FEATURES.png')\n",
    "    \n",
    "    # Plotting the ROC Curve\n",
    "    plt.figure()\n",
    "    dom1 = [fpr[0],fpr[-1]]\n",
    "    ran1 = [tpr[0],tpr[-1]]\n",
    "    plt.plot(dom1,ran1,label='Base')\n",
    "    plt.plot(fpr,tpr,label='ROC')\n",
    "    plt.legend()\n",
    "    plt.title('ROC Curve')\n",
    "    plt.savefig(name+' ROC.png')\n",
    "    f.close()\n",
    "   \n",
    "    return dom1,ran1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Engineering Easier\n",
    "__Overview:__ The Add_Dummies() function turns a specified column into a dummies dataframe and appends it to the input dataframe, deletnig the original column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Add_Dummies(df,column):\n",
    "    \n",
    "    \"\"\"\n",
    "    input:\n",
    "        df     : dataframe - the dataframe to modify\n",
    "        column : string - the column name to get the dummies for\n",
    "    \n",
    "    output:\n",
    "        df : the modified dataframe\n",
    "    \"\"\"\n",
    "    \n",
    "    dum = pd.get_dummies(df[column]) # Create a dataframe of dummies from df at the specified \n",
    "    cols = dum.columns # The column names for dum\n",
    "    \n",
    "    for i in cols: # Iterate through each of the columns\n",
    "    \n",
    "        df[column+str(i)]=dum[i] # append the corresponding dummy column to df\n",
    "    \n",
    "    df = df.drop(column,axis=1) # delete the original column from df\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost Model\n",
    "__Overview:__ The X_F() function creates a boosted forest with the given parameters and uses previously created functions to communicate results to the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def X_F(name,X_train,y_train,X_test,y_test,X,max_depth=5,n_estimators = 10,CV=False,Verbose=False,c=0):\n",
    "    \n",
    "    \"\"\"\n",
    "    input:\n",
    "        name         : String - the user specified name of the model\n",
    "        X_train      : Datframe - the train data for the model\n",
    "        y_train      : Dataframe - the labels for the train data\n",
    "        X_test       : Dataframe - the test data for the model\n",
    "        y_test       : Dataframe - the labels for the test data\n",
    "        X            : Dataframe - the entire dataset\n",
    "        max_depth    : Integer - the max depth of the generated model\n",
    "        n_estimators : Integer - the number of estimators to use in the model\n",
    "        CV           : Boolean - If true, performs a k-fold cross validation\n",
    "        Verbose      : Boolean - If true, regularaly outputs messages to inform the user what the function is doing\n",
    "        c            : Integer - dummy variable\n",
    "        \n",
    "    output:\n",
    "        A string summarizing the success/failure of the model's performance.\n",
    "    \"\"\"\n",
    "    \n",
    "    clear_output(wait=True) # Clear the output from a previous iteration, to manage whitespace\n",
    "    display(str(100*(6+c)/16)+'%') # Show how much of the function is completed\n",
    "    display('Complete\\nBuilding XGB Classifier') \n",
    "    model2 = XGBClassifier(max_depth=max_depth,n_estimators=n_estimators) # Build the model\n",
    "    \n",
    "    if CV: # If the user wants to perform a cross validation\n",
    "        clear_output(wait=True) # Clear the output from a previous iteration, to manage whitespace\n",
    "        display(str(100*(7+c)/16)+'%') # Show how much of the function is completed\n",
    "        display('Performing 7-Fold Cross Validation On Boosted Tree')\n",
    "        kfold = KFold(n_splits = 7,shuffle=True,random_state = 42) # Perform the K-fold cross validation\n",
    "        results = cross_val_score(model2,X_train,y_train,cv=kfold) # Get the cross validation score\n",
    "    \n",
    "    clear_output(wait=True) # Clear the output from a previous iteration, to manage whitespace\n",
    "    display(str(100*(8+c)/16)+'%')\n",
    "    display('Fitting Model')\n",
    "    model2.fit(X_train,y_train) # Fit the model to the train data\n",
    "  \n",
    "    \n",
    "    clear_output(wait=True) # Clear the output from a previous iteration, to manage whitespace\n",
    "    display(str(100*(9+c)/16)+'%')\n",
    "    display(' Scoring Model')\n",
    "    pred = model2.predict(X_test) # Get the predicted score of the test data on the model\n",
    "        \n",
    "    clear_output(wait=True) # Clear the output from a previous iteration, to manage whitespace\n",
    "    display(str(100*(10+c)/16)+'%')\n",
    "    display('Calculating Metrics')\n",
    "    \n",
    "    #Calculate all of the necessary metrics of the generated model to score it.\n",
    "    nM,nimportances,nindices,nScore,nfpr,ntpr,nroc_auc = calc_metrics(X_test,y_test,model2,pred,clf='XGB',roc=True)\n",
    "  \n",
    "    \n",
    "    name = 'XGBOOSTED '+name\n",
    "    \n",
    "    if Verbose: # If verbose = True then report how the model did\n",
    "        D1,R1 = Model_Report(name,y_train,nScore,nM,y_test,pred,nimportances,nindices,nfpr,ntpr,X,smote=True)\n",
    "    \n",
    "    if CV: # If CV = True then report how the cross validation performed\n",
    "        return \"Accuracy: \"+str(round(results.mean()*100,1)), \"Model Score: \"+str(round(100*accuracy_score(y_test,pred),1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting Everything Together\n",
    "__Overview:__ The run_model() function, wraps all previous functions together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(name,df,depth='11',estims='10',clf='RF',roc=True,prepre=False,XF=False,GS=False,CV=False,verbose=False):\n",
    "    \n",
    "    \"\"\"\n",
    "    input:\n",
    "        name    - String - user specified name for the model \n",
    "        df      - Dataframe - the dataset \n",
    "        depth   - Integer - the depth of the model\n",
    "        estims  - Integer - the number of estimators to use in building the model\n",
    "        clf     - String - 'RF','XGB' The type of model to build\n",
    "        roc     - Boolean - if True, calculate the roc curve\n",
    "        prepre  - Boolean - if True, prep the data using the data_prep()\n",
    "        XF      - Boolean - if True use the X_F() function\n",
    "        GS      - Boolean - if True perform a geridsearch on the model\n",
    "        CV      - Boolean - if True perform a k-fold cross validation\n",
    "        verbose - Boolean - if True, provides more data aabout the function/model prowess\n",
    "    \n",
    "    output:\n",
    "        N/A\n",
    "    \"\"\"\n",
    "    \n",
    "    display(str(100*1/16)+'%')\n",
    "    display('Prepping Data')\n",
    "    X_train,X_test,y_train,y_test,X1 = data_prep(df,prepre=prepre)\n",
    "    SX_train,SX_test,Sy_train,Sy_test,X2 = data_prep(df,smote=True,prepre=prepre)    \n",
    "    \n",
    "    clear_output(wait=True)\n",
    "    display(str(100*2/16)+'%')\n",
    "    display('Building Model')\n",
    "    model,predict = Model(X_train,X_test,y_train,y_test,depth=11,estims=10,clf=clf)\n",
    "    smodel,spredict = Model(SX_train,SX_test,Sy_train,Sy_test,depth=5,estims=35,clf=clf)\n",
    "  \n",
    "    \n",
    "    if CV:\n",
    "        clear_output(wait=True)\n",
    "        display(str(100*3/16)+'%')\n",
    "        display('Performing 7-Fold Cross Validation on Random Forest')\n",
    "        kfold = KFold(n_splits = 7,shuffle=True,random_state = 42)\n",
    "        results = cross_val_score(smodel,SX_train,Sy_train,cv=kfold)\n",
    "\n",
    "    \n",
    "    if GS and not XF:\n",
    "        \n",
    "        RF = RandomForestClassifier()\n",
    "        RF_param_grid = {'n_estimators':[15,20,25,30],\n",
    "                'max_depth':[11,13,15,17,19,21],\n",
    "                 }\n",
    "\n",
    "        RF_GS = GridSearchCV(RF, RF_param_grid, cv=4, scoring='f1', verbose=0)\n",
    "        RF_GS.fit(SX_train,np.ravel(Sy_train))\n",
    "        print(RF_GS.best_params_, RF_GS.best_score_)\n",
    "        \n",
    "        return \n",
    "    \n",
    "    elif GS and XF:\n",
    "        \n",
    "        RF = XGBClassifier()\n",
    "        RF_param_grid = {'n_estimators':[5,10,15],\n",
    "                'max_depth':[8,9,10,11],\n",
    "                 }\n",
    "\n",
    "        RF_GS = GridSearchCV(RF, RF_param_grid, cv=4, scoring='f1', verbose=0)\n",
    "        RF_GS.fit(SX_train,np.ravel(Sy_train))\n",
    "        print(RF_GS.best_params_, RF_GS.best_score_)\n",
    "        \n",
    "        return \n",
    "    \n",
    "    else:\n",
    "        clear_output(wait=True)\n",
    "        display(str(100*4/16)+'%')\n",
    "        display('Calculating Metrics')\n",
    "        M,importances,indices,Score,fpr,tpr,roc_auc = calc_metrics(X_test,y_test,model,predict,clf='RF',roc=True)\n",
    "        M2,importances2,indices2,Score2,fpr2,tpr2,roc_auc2 = calc_metrics(SX_test,Sy_test,smodel,spredict,clf='RF',roc=True)\n",
    "    \n",
    "        name1 = 'SMOTED '+name\n",
    "        name2 = 'NON-SMOTED '+name\n",
    "\n",
    "        if XF:\n",
    "            clear_output(wait=True)\n",
    "            display(str(100*5/16)+'%')\n",
    "            display('Starting Non-SMOTE XGB')\n",
    "            if CV:\n",
    "                K1,A1 = X_F(name2,X_train,y_train,X_test,y_test,X1,max_depth=5,n_estimators = 10,CV=CV)\n",
    "            \n",
    "            else:\n",
    "                X_F(name2,X_train,y_train,X_test,y_test,X1,max_depth=5,n_estimators = 10,CV=CV)\n",
    "     \n",
    "            \n",
    "            display('Starting SMOTE XGB')\n",
    "            if CV:\n",
    "                K2,A2 = X_F(name1,SX_train,Sy_train,SX_test,Sy_test,X2,max_depth=5,n_estimators = 10,CV=CV,c=1)\n",
    "                clear_output(wait=True)\n",
    "                print(\"First 7-Cross Fold Validation Score: \",K1)\n",
    "                print(A1)\n",
    "                print(\"Second 7-Cross Fold Validation Score: \",K2)\n",
    "                print(A2)\n",
    "            else:\n",
    "                X_F(name1,SX_train,Sy_train,SX_test,Sy_test,X2,max_depth=5,n_estimators = 10,CV=CV,c=11)\n",
    "\n",
    "        if verbose:\n",
    "            D1,R1 = Model_Report(name,y_train,Score,M,y_test,predict,importances,indices,fpr,tpr,X1)\n",
    "            D2,R2 = Model_Report(name,Sy_train,Score2,M2,Sy_test,spredict,importances2,indices2,fpr2,tpr2,X2,smote=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading in the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "# Reading in the .csv file\n",
    "opiates = pd.read_csv(\"./data_files/result_with_one_hot.csv\")\n",
    "\n",
    "# Opiates dataset containing all relevant features\n",
    "df = opiates[['VMONTH', 'AGE', 'SEX', 'RACE', 'ETHNIC', 'RFV1','RFV2','RFV3',\n",
    "              'RFV4','RFV5', 'DIAG1', 'REGION', 'YEAR','AGEDAYS','AGER','VDAYR',\n",
    "              'MAJOR','TEMPF','BPSYS','BPDIAS','GLUCOSE','HTIN',\n",
    "              'WTLB','BMI','BONEDENS','ETHUN','RACEUN','ETHIM','RACEIM','SUBSTED',\n",
    "              'HTWTFL','PAYTYPE','received2']] \n",
    "\n",
    "# Opiates dataset containing only features people have no control over\n",
    "#  (YEAR, DIAG1) were included for feature engineering purposes\n",
    "deterministic = opiates[['REGION','YEAR','AGE','SEX','RACE','DIAG1','ETHNIC',\n",
    "                         'TEMPF','BPSYS','BPDIAS','GLUCOSE','HTIN',\n",
    "                         'WTLB','BMI','BONEDENS','HTWTFL','received2']]\n",
    "\n",
    "# Opiates dataset containing only features people have control over\n",
    "non_deterministic = opiates[['AGE','VMONTH','RFV1','RFV2','RFV3','RFV4',\n",
    "                             'RFV5', 'DIAG1', 'REGION', 'YEAR','AGEDAYS',\n",
    "                             'VDAYR','PAYTYPE','received2']] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepping the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = generation_data(df,marching=True,start=20,end=60)\n",
    "print('df Generation Completed')\n",
    "\n",
    "det = generation_data(deterministic,marching=True,start=20,end=60)\n",
    "clear_output(wait=True)\n",
    "print('det Generation Completed')\n",
    "\n",
    "non_det = generation_data(non_deterministic,marching=True,start=20,end=60)\n",
    "clear_output(wait=True)\n",
    "print('non_det Generation Completed')\n",
    "\n",
    "df = data_prep(df,N=False,smote=False,pre=True,prepre=True)\n",
    "clear_output(wait=True)\n",
    "print('df Data Prepped')\n",
    "features = ['VMONTH','RACE','ETHNIC','RFV1','SEX','VDAYR','ETHUN','RACEUN','ETHIM','RACEIM','MAJOR','PAYTYPE']\n",
    "\n",
    "for i in tqdm(range(len(features))):\n",
    "    df = Add_Dummies(df,features[i])\n",
    "\n",
    "\n",
    "deterministic = data_prep(deterministic,N=False,smote=False,pre=True,prepre=True)\n",
    "collls = deterministic.columns\n",
    "features = ['SEX','RACE','ETHNIC',\n",
    "            'TEMPF','BPSYS','BPDIAS','GLUCOSE','BONEDENS','HTWTFL']\n",
    "clear_output(wait=True)\n",
    "for i in tqdm(range(len(features))):\n",
    "    deterministic = Add_Dummies(deterministic,features[i])\n",
    "clear_output(wait=True)\n",
    "# Opiates dataset containing only features people have control over\n",
    "features = ['VMONTH','RFV1',\n",
    "            'VDAYR','PAYTYPE']               \n",
    "non_deterministic = data_prep(non_deterministic,N=False,smote=False,pre=True,prepre=True)\n",
    "for i in tqdm(range(len(features))):\n",
    "    non_deterministic = Add_Dummies(non_deterministic,features[i])\n",
    "clear_output(wait=True)\n",
    "print('Complete')\n",
    "print(collls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name='DATASET OF DETERMINISTIC TRAITS'\n",
    "run_model(name,det,prepre=True,XF=True,GS=False,CV=True)\n",
    "# Non-XBF : max_depth = 21, n_estimators = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'DATASET OF NON DETERMINISTIC TRAITS'\n",
    "run_model(name,non_det,prepre=True,XF=True,CV=True,verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "name='THE COMPLETE DATASET'\n",
    "run_model(name,df,XF=True,CV=False)\n",
    "end = time.time()\n",
    "print('Time to Complete:',abs(start-end),' Seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Send Datasets to CSV Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('prepping')\n",
    "X_train,X_test,y_train,y_test,X = data_prep(df,smote=True,prepre=False)\n",
    "print('prepped')\n",
    "y_test=pd.DataFrame(y_test)\n",
    "print('ytest complete')\n",
    "y_train=pd.DataFrame(y_train)\n",
    "print('ytrain complete')\n",
    "print('starting ...')\n",
    "X_train.to_csv(r'./SMOTE_X_train.csv')\n",
    "print('X_train complete')\n",
    "X_test.to_csv(r'./SMOTE_X_test.csv')\n",
    "print('X_test complete')\n",
    "y_train.to_csv(r'./SMOTE_y_train.csv')\n",
    "print('y_train complete')\n",
    "y_test.to_csv(r'./SMOTE_y_test.csv')\n",
    "print('y_test complete')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
